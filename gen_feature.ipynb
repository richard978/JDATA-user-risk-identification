{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_train = pd.read_csv('data/train/uid_train.txt',sep='\\t',header=None,names=('uid','label'))\n",
    "voice_train = pd.read_csv('data/train/voice_train.txt',sep='\\t',header=None,names=('uid','opp_num','opp_head','opp_len','start_time','end_time','call_type','in_out'),dtype={'start_time':str,'end_time':str})\n",
    "sms_train = pd.read_csv('data/train/sms_train.txt',sep='\\t',header=None,names=('uid','opp_num','opp_head','opp_len','start_time','in_out'),dtype={'start_time':str})\n",
    "wa_train = pd.read_csv('data/train/wa_train.txt',sep='\\t',header=None,names=('uid','wa_name','visit_cnt','visit_dura','up_flow','down_flow','wa_type','date'),dtype={'date':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_test = pd.read_csv('data/test/voice_test_b.txt',sep='\\t',header=None,names=('uid','opp_num','opp_head','opp_len','start_time','end_time','call_type','in_out'),dtype={'start_time':str,'end_time':str})\n",
    "sms_test = pd.read_csv('data/test/sms_test_b.txt',sep='\\t',header=None,names=('uid','opp_num','opp_head','opp_len','start_time','in_out'),dtype={'start_time':str})\n",
    "wa_test = pd.read_csv('data/test/wa_test_b.txt',sep='\\t',header=None,names=('uid','wa_name','visit_cnt','visit_dura','up_flow','down_flow','wa_type','date'),dtype={'date':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_test = pd.DataFrame({'uid':pd.unique(wa_test['uid'])})\n",
    "uid_test.to_csv('ref/uid_test_a.txt',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice = pd.concat([voice_train,voice_test],axis=0)\n",
    "sms = pd.concat([sms_train,sms_test],axis=0)\n",
    "wa = pd.concat([wa_train,wa_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['start_time'] = sms['start_time'].apply(lambda x:str(x).zfill(8) if len(str(x))<8 else str(x))\n",
    "sms['start_date'] = sms['start_time'].apply(lambda x:int(x[0:2]))\n",
    "sms['start_time'] = sms['start_time'].apply(lambda x:datetime.strptime(\"{}:{}:{}\".format(x[2:4],x[4:6],x[6:8]),\"%H:%M:%S\"))\n",
    "sms['start_hour'] = sms['start_time'].apply(lambda x:x.hour)\n",
    "sms['is_work_time'] = sms['start_hour'].apply(lambda x:1 if (8<=x)&(x<=18) else 0)\n",
    "sms['is_long_head'] = sms['opp_head'].apply(lambda x:1 if len(str(x))==3 else 0)\n",
    "sms['is_long_len'] = sms['opp_len'].apply(lambda x:1 if x>5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "encoded = le.fit_transform(list(set(sms['opp_num'])))\n",
    "in_encoded = le.inverse_transform(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sms_feat(sms):\n",
    "    sms_feature = pd.DataFrame()\n",
    "\n",
    "    sms_gp = sms.groupby('uid')['uid']\n",
    "    cnt = sms_gp.apply(lambda x:x.count())\n",
    "    sms_feature['uid'] = cnt.index\n",
    "    sms_feature['sms_cnt'] = cnt.values\n",
    "\n",
    "    \n",
    "    sms_gp = sms.groupby('uid')['opp_num']\n",
    "    cnt_num = sms_gp.apply(lambda x:len(set(x)))\n",
    "    sms_feature['sms_cnt_num'] = cnt_num.values\n",
    "    \n",
    "    '''\n",
    "    sms_gp = sms.groupby('uid')['opp_num']\n",
    "    cnt_num = sms_gp.apply(lambda x:x.value_counts().head(1)).reset_index()\n",
    "    most_appear_num = []\n",
    "    for i in range(0,len(cnt_num)):\n",
    "        idx = np.where(in_encoded == cnt_num['level_1'].values[i])\n",
    "        lb = encoded[idx]\n",
    "        most_appear_num.append(lb[0])\n",
    "    m = pd.DataFrame({'sms_mostfreq_num':most_appear_num})\n",
    "    m.insert(0,'uid',cnt.index)\n",
    "    sms_feature = sms_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "    \n",
    "    sms_gp = sms.groupby('uid')['opp_num']\n",
    "    cnt_num = sms_gp.apply(lambda x:x.value_counts().head(1)).reset_index()\n",
    "    most_appear_count = []\n",
    "    for i in range(0,len(cnt_num)):\n",
    "        most_appear_count.append(cnt_num['opp_num'].values[i])\n",
    "    m = pd.DataFrame({'sms_mostfreq_count':most_appear_count})\n",
    "    m.insert(0,'uid',cnt.index)\n",
    "    sms_feature = sms_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "    '''\n",
    "    \n",
    "\n",
    "    sms_gp = sms.groupby('uid')['opp_head']\n",
    "    cnt_num = sms_gp.apply(lambda x:len(set(x)))\n",
    "    sms_feature['sms_head_cnt'] = cnt_num.values\n",
    "    \n",
    "    sms_gp = sms.groupby('uid')['opp_head'].agg(['max','min'])\n",
    "    m = sms_gp.add_prefix('sms_head_').reset_index().fillna(0)\n",
    "    sms_feature = sms_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "    \n",
    "    sms_gp = sms.groupby('uid')['opp_head']\n",
    "    cnt_num = sms_gp.apply(lambda x:mode(x)[0][0])\n",
    "    sms_feature['sms_head_mode'] = cnt_num.values\n",
    "    \n",
    "    sms_gp = sms.groupby(['uid','is_long_head'])['opp_head']\n",
    "    cnt_hd = sms_gp.apply(lambda x:x.count())\n",
    "    cnt_hd = cnt_hd.unstack(fill_value=0).reset_index(drop=True)\n",
    "    cnt_hd.columns = ['0', '1']\n",
    "    sms_feature['sms_cnt_long_head'] = cnt_hd['1']\n",
    "    sms_feature['sms_cnt_not_long_head'] = cnt_hd['0']\n",
    "    \n",
    "\n",
    "    '''\n",
    "    sms_gp = sms.groupby('uid')['start_date']\n",
    "    cnt_date = sms_gp.apply(lambda x:mode(x)[0][0])\n",
    "    sms_feature['sms_mode_date'] = cnt_date.values\n",
    "\n",
    "    sms_gp = sms.groupby('uid')['start_time']\n",
    "    cnt_time = sms_gp.apply(lambda x:len(set(x)))\n",
    "    sms_feature['sms_cnt_time'] = cnt_time.values\n",
    "    '''\n",
    "    \n",
    "    sms_gp = sms.groupby(['uid','is_work_time'])['start_time']\n",
    "    cnt_work = sms_gp.apply(lambda x:x.count())\n",
    "    cnt_work = cnt_work.unstack(fill_value=0).reset_index(drop=True)\n",
    "    cnt_work.columns = ['0', '1']\n",
    "    sms_feature['sms_cnt_work'] = cnt_work['1']\n",
    "    sms_feature['sms_cnt_not_work'] = cnt_work['0']\n",
    "    \n",
    "    \n",
    "    sms_gp = sms.groupby('uid')['opp_len']\n",
    "    cnt_num = sms_gp.apply(lambda x:len(set(x)))\n",
    "    sms_feature['sms_opp_len_cnt'] = cnt_num.values\n",
    "    \n",
    "    sms_gp = sms.groupby('uid')['opp_len'].agg(['std','max','min','mean','median'])\n",
    "    m = sms_gp.add_prefix('sms_opp_len_').reset_index().fillna(0)\n",
    "    sms_feature = sms_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "    \n",
    "    sms_gp = sms.groupby('uid')['opp_len']\n",
    "    cnt_num = sms_gp.apply(lambda x:mode(x)[0][0])\n",
    "    sms_feature['sms_opp_len_mode'] = cnt_num.values\n",
    "    \n",
    "    '''\n",
    "    sms_gp = sms.groupby(['uid','is_long_len'])['opp_len']\n",
    "    cnt_len = sms_gp.apply(lambda x:x.count())\n",
    "    cnt_len = cnt_len.unstack().add_prefix('sms_cnt_len_').reset_index().fillna(0)\n",
    "    sms_feature = sms_feature.merge(cnt_len, on='uid', how='left').reset_index(drop=True)\n",
    "    \n",
    "    sms_gp = sms.groupby(['uid','opp_len'])['uid']\n",
    "    cnt_len = sms_gp.apply(lambda x:x.count())\n",
    "    cnt_len = cnt_len.unstack().add_prefix('sms_opp_len_').reset_index().fillna(0)\n",
    "    sms_feature = sms_feature.merge(cnt_len, on='uid', how='left').reset_index(drop=True)\n",
    "    \n",
    "    sms_gp = sms.groupby(['uid','in_out'])['opp_num']\n",
    "    cnt_io = sms_gp.apply(lambda x:x.count())\n",
    "    cnt_io = cnt_io.unstack(fill_value=0).reset_index(drop=True)\n",
    "    cnt_io.columns = ['0', '1']\n",
    "    sms_feature['sms_cnt_out'] = cnt_io['0']\n",
    "    sms_feature['sms_cnt_in'] = cnt_io['1']\n",
    "    '''\n",
    "\n",
    "    return sms_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feature = pd.DataFrame()\n",
    "\n",
    "for i in (0,16,32):  \n",
    "    sms_split = sms[(i<=sms['start_date'])&(sms['start_date']<i+16)]\n",
    "    for j in (0,1):\n",
    "        sms_split_io = sms_split[sms_split['in_out']==j]\n",
    "        sms_gp = sms_split_io.groupby('uid')['uid']\n",
    "        cnt = sms_gp.apply(lambda x:x.count())\n",
    "        if(j==0):\n",
    "            sms_feature = sms_feat(sms_split_io).iloc[:,1:].add_suffix(\"_day\"+str(i)+\"_out\")\n",
    "        else:\n",
    "            sms_feature = sms_feat(sms_split_io).iloc[:,1:].add_suffix(\"_day\"+str(i)+\"_in\")\n",
    "        sms_feature.insert(0,'uid',cnt.index)\n",
    "        if i==0 and j==0:\n",
    "            total_feature = sms_feature\n",
    "        else:\n",
    "            total_feature = total_feature.merge(sms_feature, on='uid', how='outer').reset_index(drop=True)\n",
    "\n",
    "total_feature = total_feature.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice['opp_head'] = voice['opp_head'].replace('DDD',0)\n",
    "voice['opp_head'] = voice['opp_head'].apply(lambda x:int(x))\n",
    "voice['start_time'] = voice['start_time'].apply(lambda x:str(x).zfill(8) if len(str(x))<8 else str(x))\n",
    "voice['start_date'] = voice['start_time'].apply(lambda x:int(x[0:2]))\n",
    "voice['start_time'] = voice['start_time'].apply(lambda x:datetime.strptime(\"{}:{}:{}\".format(x[2:4],x[4:6],x[6:8]),\"%H:%M:%S\"))\n",
    "voice['end_time'] = voice['end_time'].apply(lambda x:str(x).zfill(8) if len(str(x))<8 else str(x))\n",
    "voice['end_time'] = voice['end_time'].apply(lambda x:datetime.strptime(\"{}:{}:{}\".format(x[2:4],x[4:6],x[6:8]),\"%H:%M:%S\"))\n",
    "voice['start_hour'] = voice['start_time'].apply(lambda x:x.hour)\n",
    "voice['is_work_time'] = voice['start_hour'].apply(lambda x:1 if (8<=x)&(x<=18) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice['time_dura'] = voice['end_time'] - voice['start_time']\n",
    "voice['time_dura'] = voice['time_dura'].apply(lambda x:x.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voice_feat(voice):\n",
    "    voice_feature = pd.DataFrame()\n",
    "\n",
    "    voice_gp = voice.groupby('uid')['uid']\n",
    "    cnt = voice_gp.apply(lambda x:x.count())\n",
    "    voice_feature['uid'] = cnt.index\n",
    "    voice_feature['voice_cnt'] = cnt.values\n",
    "\n",
    "    voice_gp = voice.groupby('uid')['opp_num']\n",
    "    cnt_num = voice_gp.apply(lambda x:len(set(x)))\n",
    "    voice_feature['voice_cnt_num'] = cnt_num.values\n",
    "\n",
    "    voice_gp = voice.groupby('uid')['opp_head']\n",
    "    cnt_num = voice_gp.apply(lambda x:len(set(x)))\n",
    "    voice_feature['voice_head_cnt'] = cnt_num.values\n",
    "    \n",
    "    voice_gp = voice.groupby('uid')['opp_head'].agg(['max','min'])\n",
    "    m = voice_gp.add_prefix('voice_head_').reset_index().fillna(0)\n",
    "    voice_feature = voice_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "    \n",
    "    voice_gp = voice.groupby('uid')['opp_head']\n",
    "    cnt_num = voice_gp.apply(lambda x:mode(x)[0][0])\n",
    "    voice_feature['voice_head_mode'] = cnt_num.values\n",
    "    \n",
    "    voice_gp = voice.groupby('uid')['time_dura'].agg(['std','max','min','mean','median','sum'])\n",
    "    m = voice_gp.add_prefix('voice_dura_').reset_index().fillna(0)\n",
    "    voice_feature = voice_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "    \n",
    "    voice_gp = voice.groupby(['uid','is_work_time'])['opp_num']\n",
    "    cnt_work = voice_gp.apply(lambda x:x.count())\n",
    "    cnt_work = cnt_work.unstack(fill_value=0).reset_index(drop=True)\n",
    "    cnt_work.columns = ['0', '1']\n",
    "    voice_feature['voice_cnt_work'] = cnt_work['0']\n",
    "    voice_feature['vocie_cnt_not_work'] = cnt_work['1']\n",
    "    \n",
    "    voice_gp = voice.groupby('uid')['opp_len']\n",
    "    cnt_num = voice_gp.apply(lambda x:len(set(x)))\n",
    "    voice_feature['voice_opp_len_cnt'] = cnt_num.values\n",
    "    \n",
    "    voice_gp = voice.groupby('uid')['opp_len'].agg(['std','max','min','mean','median'])\n",
    "    m = voice_gp.add_prefix('voice_opp_len_').reset_index().fillna(0)\n",
    "    voice_feature = voice_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "    \n",
    "    voice_gp = voice.groupby('uid')['opp_len']\n",
    "    cnt_num = voice_gp.apply(lambda x:mode(x)[0][0])\n",
    "    voice_feature['voice_opp_len_mode'] = cnt_num.values\n",
    "\n",
    "    '''\n",
    "    voice_gp = voice.groupby(['uid','opp_len'])['uid']\n",
    "    cnt_len = voice_gp.apply(lambda x:x.count())\n",
    "    cnt_len = cnt_len.unstack().add_prefix('voice_opp_len_').reset_index().fillna(0)\n",
    "    voice_feature = voice_feature.merge(cnt_len, on='uid', how='left').reset_index(drop=True)\n",
    "    \n",
    "    voice_gp = voice.groupby(['uid','in_out'])['opp_num']\n",
    "    cnt_io = voice_gp.apply(lambda x:x.count())\n",
    "    cnt_io = cnt_io.unstack(fill_value=0).reset_index(drop=True)\n",
    "    cnt_io.columns = ['0', '1']\n",
    "    voice_feature['voice_cnt_out'] = cnt_io['0']\n",
    "    voice_feature['voice_cnt_in'] = cnt_io['1']\n",
    "    '''\n",
    "\n",
    "    voice_gp = voice.groupby(['uid','call_type'])['opp_num']\n",
    "    cnt_type = voice_gp.apply(lambda x:x.count())\n",
    "    cnt_type = cnt_type.unstack().add_prefix('voice_cnt_type').reset_index().fillna(0)\n",
    "    voice_feature = voice_feature.merge(cnt_type, on='uid', how='left').reset_index(drop=True)\n",
    "\n",
    "    return voice_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (0,16,32):\n",
    "    voice_split = voice[(i<=voice['start_date'])&(voice['start_date']<i+16)]\n",
    "    for j in (0,1):\n",
    "        voice_split_io = voice_split[voice_split['in_out']==j]\n",
    "        voice_gp = voice_split_io.groupby('uid')['uid']\n",
    "        cnt = voice_gp.apply(lambda x:x.count())\n",
    "        if(j==0):\n",
    "            voice_feature = voice_feat(voice_split_io).iloc[:,1:].add_suffix(\"_day\"+str(i)+\"_out\")\n",
    "        else:\n",
    "            voice_feature = voice_feat(voice_split_io).iloc[:,1:].add_suffix(\"_day\"+str(i)+\"_in\")\n",
    "        voice_feature.insert(0,'uid',cnt.index)\n",
    "        total_feature = total_feature.merge(voice_feature, on='uid', how='outer').reset_index(drop=True)\n",
    "\n",
    "total_feature = total_feature.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa['flow_per_sec'] =(wa['up_flow'] + wa['down_flow'])/wa['visit_dura']\n",
    "wa['flow_per_sec'] = wa['flow_per_sec'].replace(np.inf,0)\n",
    "wa['dura_per_visit'] = wa['visit_dura']/wa['visit_cnt']\n",
    "wa['visit_dura'] = wa['visit_dura'].fillna(0)\n",
    "wa['up_flow'] =  wa['up_flow'].fillna(0)\n",
    "wa['down_flow'] =  wa['down_flow'].fillna(0)\n",
    "wa['flow_per_sec'] = wa['flow_per_sec'].fillna(0)\n",
    "wa['date'] = wa['date'].fillna(0)\n",
    "wa['date'] = wa['date'].apply(lambda x:int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wa_feat(wa): \n",
    "    wa_feature = pd.DataFrame()\n",
    "\n",
    "    wa_gp = wa.groupby('uid')['uid']\n",
    "    cnt = wa_gp.apply(lambda x:x.count())\n",
    "    wa_feature['uid'] = cnt.index\n",
    "    wa_feature['wa_cnt'] = cnt.values\n",
    "    \n",
    "    wa_gp = wa.groupby('uid')['wa_name']\n",
    "    cnt = wa_gp.apply(lambda x:len(set(x)))\n",
    "    wa_feature['wa_name_cnt'] = cnt.values\n",
    "    \n",
    "    wa_gp = wa.groupby('uid')['visit_cnt'].agg(['std','max','min','mean','median','sum'])\n",
    "    m = wa_gp.add_prefix('wa_visit_cnt_').reset_index().fillna(0)\n",
    "    wa_feature = wa_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "\n",
    "    wa_gp = wa.groupby('uid')['visit_dura'].agg(['std','max','min','mean','median','sum'])\n",
    "    m = wa_gp.add_prefix('wa_visit_dura_').reset_index().fillna(0)\n",
    "    wa_feature = wa_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "    \n",
    "    wa_gp = wa.groupby('uid')['dura_per_visit'].agg(['std','max','min','mean','median'])\n",
    "    m = wa_gp.add_prefix('dura_per_visit_').reset_index().fillna(0)\n",
    "    wa_feature = wa_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "\n",
    "    wa_gp = wa.groupby('uid')['up_flow'].agg(['std','max','min','mean','median','sum'])\n",
    "    m = wa_gp.add_prefix('wa_up_flow_dura_').reset_index().fillna(0)\n",
    "    wa_feature = wa_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "\n",
    "    wa_gp = wa.groupby('uid')['down_flow'].agg(['std','max','min','mean','median','sum'])\n",
    "    m = wa_gp.add_prefix('wa_down_flow_dura_').reset_index().fillna(0)\n",
    "    wa_feature = wa_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "\n",
    "    wa_gp = wa.groupby('uid')['flow_per_sec'].agg(['std','max','min','mean','median'])\n",
    "    m = wa_gp.add_prefix('wa_flow_per_sec_').reset_index().fillna(0)\n",
    "    wa_feature = wa_feature.merge(m, on='uid', how='left').reset_index(drop=True)\n",
    "\n",
    "    '''\n",
    "    wa_gp = wa.groupby(['uid','wa_type'])['uid']\n",
    "    cnt_type = wa_gp.apply(lambda x:x.count())\n",
    "    cnt_type = cnt_type.unstack(fill_value=0).reset_index(drop=True)\n",
    "    cnt_type.columns = ['0', '1']\n",
    "    wa_feature['wa_count_type0'] = cnt_type['0']\n",
    "    wa_feature['wa_count_type1'] = cnt_type['1']\n",
    "\n",
    "    wa_wx = wa[wa['wa_name']=='微信']\n",
    "    wa_gp = wa_wx.groupby('uid')['wa_name']\n",
    "    cnt_wx = wa_gp.apply(lambda x:x.count())\n",
    "    cnt_wx = cnt_wx.to_frame().reset_index()\n",
    "    cnt_wx.columns = ['uid','wa_count_wx']\n",
    "    wa_feature = wa_feature.merge(cnt_wx, on='uid', how='left').reset_index(drop=True)\n",
    "    '''\n",
    "    \n",
    "    return wa_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in (0,16,32):\n",
    "    wa_split = wa[(i<=wa['date'])&(wa['date']<i+16)]\n",
    "    for j in (0,1):     \n",
    "        wa_split_io = wa_split[wa_split['wa_type']==j]\n",
    "        wa_gp = wa_split_io.groupby('uid')['uid']\n",
    "        cnt = wa_gp.apply(lambda x:x.count())\n",
    "        if(j==0):\n",
    "            wa_feature = wa_feat(wa_split_io).iloc[:,1:].add_suffix(\"_day\"+str(i)+\"_type0\")\n",
    "        else:\n",
    "            wa_feature = wa_feat(wa_split_io).iloc[:,1:].add_suffix(\"_day\"+str(i)+\"_type1\")\n",
    "        wa_feature.insert(0,'uid',cnt.index)\n",
    "        total_feature = total_feature.merge(wa_feature, on='uid', how='outer').reset_index(drop=True)\n",
    "\n",
    "total_feature = total_feature.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca = PCA(n_components=100)\n",
    "#new_pca = pca.fit_transform(total_feature.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_feature.iloc[:,1:] = preprocessing.scale(total_feature.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_feature.iloc[:,1:] = Normalizer().fit_transform(total_feature.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = uid_train\n",
    "train_feature = pd.merge(train_feature,total_feature,how='left',on='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature = uid_test\n",
    "test_feature = pd.merge(test_feature,total_feature,how='left',on='uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature.to_csv('train_feature.csv',index=None)\n",
    "test_feature.to_csv('test_feature.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(total_feature.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
